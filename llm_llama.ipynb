{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "m3m5msf5lEx2"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import urllib.request"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "MASTER_CONFIG = {\n",
        "    \"vocab_size\": 65,       # Number of unique characters in the dataset\n",
        "    \"batch_size\": 32,       # Number of batches\n",
        "    \"context_window\": 16,   # Number of characters in a batch\n",
        "    \"d_model\": 128,         # Dimension of linear layers\n",
        "    \"epochs\": 10_000,       # Number of training epochs\n",
        "    \"log_interval\": 100,    # Frequency of logging the loss in epochs\n",
        "    \"n_heads\": 8            # Number of attention heads\n",
        "}"
      ],
      "metadata": {
        "id": "VBCJ9JTTlJbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "hDzje7UalLRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tiny Shakespeare\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "file_name = \"tinyshakespeare.txt\"\n",
        "urllib.request.urlretrieve(url, file_name)"
      ],
      "metadata": {
        "id": "3n4ItxJ5lMGV",
        "outputId": "f37283f3-9a56-4660-e148-0973d40edcd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tinyshakespeare.txt', <http.client.HTTPMessage at 0x7d609df323e0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocab list of unique chars\n",
        "lines = open(\"tinyshakespeare.txt\", 'r').read()\n",
        "vocab = sorted(list(set(lines)))"
      ],
      "metadata": {
        "id": "Qbb_QEirlPHT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Int to string\n",
        "itos = {i: ch for i, ch in enumerate(vocab)}\n",
        "\n",
        "# String to int\n",
        "stoi = {ch: i for i, ch in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "fVWOD7M2lQqx"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding and decoding\n",
        "def encode(s):\n",
        "    return [stoi[ch] for ch in s]\n",
        "\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "7xxeMFullSah"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataset into a tensor\n",
        "dataset = torch.tensor(encode(lines), dtype=torch.int8)"
      ],
      "metadata": {
        "id": "GDwuDSKDlWnf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into train/val/test batches\n",
        "def get_batches(data, split, batch_size, context_window, config=MASTER_CONFIG):\n",
        "  # Train/val/test split = 0.8/0.1/0.1\n",
        "  train = data[:int(0.8 * len(data))]\n",
        "  val = data[int(0.8 * len(data)):int(0.9 * len(data))]\n",
        "  test = data[int(0.9 * len(data)):]\n",
        "\n",
        "  # Determine which batch to use\n",
        "  batch_data = train\n",
        "  if split == \"val\":\n",
        "    batch_data = val\n",
        "  elif split == \"test\":\n",
        "    batch_data = test\n",
        "\n",
        "  # batch_size number of random starting points in the data\n",
        "  ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
        "\n",
        "  # Input and target sequences\n",
        "  x = torch.stack([batch_data[i:i + context_window] for i in ix]).long()\n",
        "  y = torch.stack([batch_data[i + 1:i + context_window + 1] for i in ix]).long()\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "V9-olP70lYU3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "id": "jSn1oXjVla5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes the mean loss for 10 batches for train/val\n",
        "@torch.no_grad()\n",
        "def evaluate_loss(model, config=MASTER_CONFIG):\n",
        "  out = {}\n",
        "\n",
        "  # Set the model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Iterate through train and val splits\n",
        "  for split in [\"train\", \"val\"]:\n",
        "    losses = []\n",
        "\n",
        "    # Get 10 sample batches\n",
        "    for _ in range(10):\n",
        "      # Input and target sequences\n",
        "      xb, yb = get_batches(dataset, split, config[\"batch_size\"], config[\"context_window\"])\n",
        "      # Run the model and calculate the loss\n",
        "      _, loss = model(xb, yb)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "    out[split] = np.mean(losses)\n",
        "\n",
        "  # Set the model to train mode\n",
        "  model.train()\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "Z0VujPjxlbdp"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama Model"
      ],
      "metadata": {
        "id": "dfuzrPk6ld6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, layer_shape, eps=1e-8, bias=False):\n",
        "    super(RMSNorm, self).__init__()\n",
        "\n",
        "    # Register a learnable parameter \"scale\" as a part of the module\n",
        "    self.register_parameter(\"scale\", nn.Parameter(torch.ones(layer_shape)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (batch, seq_len, d_model)\n",
        "\n",
        "    # Calculate Frobenius norm: RMS = 1/sqrt(N) * Frobenius norm\n",
        "    ff_rms = torch.linalg.norm(x, dim=(1, 2)) * x[0].numel() ** -0.5\n",
        "\n",
        "    # Normalize x with respect to RMS\n",
        "    raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "    # Scale the normalized tensor using the learnable parameter \"scale\"\n",
        "    return self.scale[:x.shape[1], :].unsqueeze(0) * raw"
      ],
      "metadata": {
        "id": "LHEVqscNl0Hq"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rotary_matrix(context_window, embedding_dim):\n",
        "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
        "\n",
        "    # Loop through each position in the context window\n",
        "    for position in range(context_window):\n",
        "      # Loop through each dimension in the embedding\n",
        "      for i in range(embedding_dim // 2):\n",
        "        # Calculate the rotation angle based on the position and embedding dimension\n",
        "        theta = 10_000 ** (-2 * (i - 1) / embedding_dim)\n",
        "        # Calculate the rotated matrix elements\n",
        "        m_theta = position * theta\n",
        "        R[position, 2 * i, 2 * i] = np.cos(m_theta)\n",
        "        R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)\n",
        "        R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)\n",
        "        R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)\n",
        "\n",
        "    return R\n",
        "\n",
        "class RoPEMaskedAttentionHead(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    # Linear transformation for query\n",
        "    self.w_q = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "    # Linear transformation for key\n",
        "    self.w_k = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "    # Linear transformation for value\n",
        "    self.w_v = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "    # Obtain rotary matrix for positional embeddings\n",
        "    self.R = get_rotary_matrix(config['context_window'], config['d_model'])\n",
        "\n",
        "  def forward(self, x, return_attn_weights=False):\n",
        "    # x: (batch, seq_len, d_model)\n",
        "    b, m, d = x.shape\n",
        "\n",
        "    # Linear transformations for Q, K, and V\n",
        "    q = self.w_q(x)\n",
        "    k = self.w_k(x)\n",
        "    v = self.w_v(x)\n",
        "\n",
        "    # Rotate Q and K using the RoPE matrix\n",
        "    q_rotated = (torch.bmm(q.transpose(0, 1), self.R[:m])).transpose(0, 1)\n",
        "    k_rotated = (torch.bmm(k.transpose(0, 1), self.R[:m])).transpose(0, 1)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    activations = F.scaled_dot_product_attention(q_rotated, k_rotated, v, dropout_p=0.1, is_causal=True)\n",
        "\n",
        "    # Return the attention weights\n",
        "    if return_attn_weights:\n",
        "      # Create causal attention mask\n",
        "      attn_mask = torch.tril(torch.ones((m, m)), diagonal=0)\n",
        "      # Calculate attention weights and add causal mask\n",
        "      attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1, 2)) / np.sqrt(d) + attn_mask\n",
        "      attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "      return activations, attn_weights\n",
        "\n",
        "    return activations"
      ],
      "metadata": {
        "id": "FOza2XT3rmkV"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoPEMaskedMultiheadAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    # RoPEMaskedAttentionHead instances as attention heads\n",
        "    self.heads = nn.ModuleList([RoPEMaskedAttentionHead(config) for _ in range(config[\"n_heads\"])])\n",
        "    # Linear layer after concatenating the heads\n",
        "    self.linear = nn.Linear(config[\"n_heads\"] * config[\"d_model\"], config[\"d_model\"])\n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (batch, seq_len, d_model)\n",
        "\n",
        "    # Process each attention head and concatenate the results\n",
        "    heads = [h(x) for h in self.heads]\n",
        "    x = torch.cat(heads, dim=-1)\n",
        "    # Linear layer\n",
        "    x = self.linear(x)\n",
        "    # Dropout layer\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "tw7ob6kHyb3C"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwiGLU(nn.Module):\n",
        "  def __init__(self, size):\n",
        "    super().__init__()\n",
        "\n",
        "    # Linear transformation for the gate mechanism\n",
        "    self.linear_gate = nn.Linear(size, size)\n",
        "    # Linear transformation for the main branch\n",
        "    self.linear = nn.Linear(size, size)\n",
        "    # Randomly init the beta parameter\n",
        "    self.beta = torch.randn(1, requires_grad=True)\n",
        "\n",
        "    # Register beta as a learnable parameter\n",
        "    self.beta = nn.Parameter(torch.ones())\n",
        "    self.register_parameter(\"beta\", self.beta)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Swish-Gated Linear Unit computation\n",
        "    swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))\n",
        "    # Element-wise multiplication between the gate and main branch\n",
        "    out = swish_gate * self.linear(x)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "9jRyJHtD2ont"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Llama(nn.Module):\n",
        "  def __init__(self, config=MASTER_CONFIG):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    # Embedding layer to convert character indices to vectors\n",
        "    self.embedding = nn.Embedding(config[\"vocab_size\"], config[\"d_model\"])\n",
        "    # RMSNorm layer for pre-normalization\n",
        "    self.rms = RMSNorm((config[\"context_window\"], config[\"d_model\"]))\n",
        "    # RoPEMaskedMultiheadAttention layer\n",
        "    self.rope_attention = RoPEMaskedMultiheadAttention(config)\n",
        "    # Linear layers\n",
        "    self.linear = nn.Sequential(\n",
        "        nn.Linear(config[\"d_model\"], config[\"d_model\"]),\n",
        "        SwiGLU(config['d_model'])\n",
        "    )\n",
        "\n",
        "    # Final linear layer for prediction\n",
        "    self.last_linear = nn.Linear(config[\"d_model\"], config[\"vocab_size\"])\n",
        "\n",
        "    print(\"# of params:\", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Embedding layer converts character indices to vectors\n",
        "    x = self.embedding(idx)\n",
        "\n",
        "    # Attention block\n",
        "    x = self.rms(x)\n",
        "    x = x + self.rope_attention(x)\n",
        "\n",
        "    # Linear block\n",
        "    x = self.rms(x)\n",
        "    x = x + self.linear(x)\n",
        "    logits = self.last_linear(x)\n",
        "\n",
        "    # If there are targets, calculate and return the cross entropy loss\n",
        "    if targets is not None:\n",
        "      loss = F.cross_entropy(logits.view(-1, self.config[\"vocab_size\"]), targets.view(-1))\n",
        "      return logits, loss\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "Bisbawb6lfgd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(model, optimizer, scheduler=None, config=MASTER_CONFIG, print_logs=False):\n",
        "  losses = []\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Iterate through epochs\n",
        "  for epoch in range(config[\"epochs\"]):\n",
        "    # Zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Get training batches\n",
        "    xs, ys = get_batches(dataset, \"train\", config[\"batch_size\"], config[\"context_window\"])\n",
        "    # Run the model and calculate the loss\n",
        "    logits, loss = model(xs, targets=ys)\n",
        "    # Backpropagate the loss\n",
        "    loss.backward()\n",
        "    # Step the optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    # If there's a learning rate scheduler, adjust the learning rate\n",
        "    if scheduler:\n",
        "      scheduler.step()\n",
        "\n",
        "    # Log progress\n",
        "    if epoch % config[\"log_interval\"] == 0:\n",
        "      # Calculate batch time\n",
        "      batch_time = time.time() - start_time\n",
        "\n",
        "      # Evaluate loss\n",
        "      x = evaluate_loss(model)\n",
        "      losses += [x]\n",
        "\n",
        "      # Print progress\n",
        "      if print_logs:\n",
        "        print(f\"Epoch: {epoch} | Train loss: {x['train']:0.3f} | Val loss: {x['val']:0.3f} | Time: {batch_time:0.3f} | ETA (sec): {batch_time * (config['epochs'] - epoch) / config['log_interval']:0.3f}\")\n",
        "\n",
        "      # Reset timer\n",
        "      start_time = time.time()\n",
        "\n",
        "      # Print learning rate\n",
        "      if scheduler:\n",
        "        print(\"Learning rate:\", scheduler.get_last_lr())\n",
        "\n",
        "  print(\"Val loss:\", losses[-1][\"val\"])\n",
        "\n",
        "  return pd.DataFrame(losses).plot()"
      ],
      "metadata": {
        "id": "AoLKTPIEljau"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinit model and optimizer\n",
        "model = Llama(MASTER_CONFIG)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Train model\n",
        "train(model, optimizer, print_logs=True)"
      ],
      "metadata": {
        "id": "qcfSmKcbll9v",
        "outputId": "fb8f104f-be99-4236-cc06-72559fe7ccd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of params: 2132545\n",
            "Epoch: 0 | Train loss: 3.894 | Val loss: 3.882 | Time: 0.230 | ETA in sec 23.028\n",
            "Epoch: 10 | Train loss: 2.915 | Val loss: 2.925 | Time: 1.985 | ETA in sec 196.507\n",
            "Epoch: 20 | Train loss: 2.693 | Val loss: 2.769 | Time: 1.961 | ETA in sec 192.190\n",
            "Epoch: 30 | Train loss: 2.622 | Val loss: 2.595 | Time: 2.793 | ETA in sec 270.898\n",
            "Epoch: 40 | Train loss: 2.551 | Val loss: 2.574 | Time: 1.946 | ETA in sec 186.781\n",
            "Epoch: 50 | Train loss: 2.464 | Val loss: 2.515 | Time: 1.943 | ETA in sec 184.624\n",
            "Epoch: 60 | Train loss: 2.444 | Val loss: 2.518 | Time: 1.929 | ETA in sec 181.327\n",
            "Epoch: 70 | Train loss: 2.440 | Val loss: 2.445 | Time: 2.825 | ETA in sec 262.698\n",
            "Epoch: 80 | Train loss: 2.393 | Val loss: 2.441 | Time: 1.950 | ETA in sec 179.359\n",
            "Epoch: 90 | Train loss: 2.340 | Val loss: 2.387 | Time: 1.924 | ETA in sec 175.123\n",
            "Epoch: 100 | Train loss: 2.356 | Val loss: 2.348 | Time: 1.923 | ETA in sec 173.108\n",
            "Epoch: 110 | Train loss: 2.324 | Val loss: 2.347 | Time: 2.597 | ETA in sec 231.165\n",
            "Epoch: 120 | Train loss: 2.262 | Val loss: 2.342 | Time: 2.414 | ETA in sec 212.389\n",
            "Epoch: 130 | Train loss: 2.277 | Val loss: 2.327 | Time: 1.956 | ETA in sec 170.137\n",
            "Epoch: 140 | Train loss: 2.302 | Val loss: 2.339 | Time: 2.643 | ETA in sec 227.314\n",
            "Epoch: 150 | Train loss: 2.272 | Val loss: 2.309 | Time: 1.926 | ETA in sec 163.692\n",
            "Epoch: 160 | Train loss: 2.231 | Val loss: 2.287 | Time: 2.758 | ETA in sec 231.698\n",
            "Epoch: 170 | Train loss: 2.229 | Val loss: 2.264 | Time: 2.029 | ETA in sec 168.378\n",
            "Epoch: 180 | Train loss: 2.197 | Val loss: 2.269 | Time: 2.371 | ETA in sec 194.449\n",
            "Epoch: 190 | Train loss: 2.215 | Val loss: 2.260 | Time: 1.935 | ETA in sec 156.741\n",
            "Epoch: 200 | Train loss: 2.209 | Val loss: 2.309 | Time: 1.924 | ETA in sec 153.898\n",
            "Epoch: 210 | Train loss: 2.230 | Val loss: 2.251 | Time: 2.284 | ETA in sec 180.429\n",
            "Epoch: 220 | Train loss: 2.138 | Val loss: 2.225 | Time: 2.093 | ETA in sec 163.259\n",
            "Epoch: 230 | Train loss: 2.169 | Val loss: 2.225 | Time: 1.934 | ETA in sec 148.894\n",
            "Epoch: 240 | Train loss: 2.143 | Val loss: 2.178 | Time: 1.984 | ETA in sec 150.772\n",
            "Epoch: 250 | Train loss: 2.177 | Val loss: 2.189 | Time: 2.528 | ETA in sec 189.569\n",
            "Epoch: 260 | Train loss: 2.173 | Val loss: 2.234 | Time: 1.968 | ETA in sec 145.617\n",
            "Epoch: 270 | Train loss: 2.149 | Val loss: 2.210 | Time: 1.962 | ETA in sec 143.237\n",
            "Epoch: 280 | Train loss: 2.116 | Val loss: 2.231 | Time: 1.930 | ETA in sec 138.956\n",
            "Epoch: 290 | Train loss: 2.099 | Val loss: 2.206 | Time: 2.878 | ETA in sec 204.303\n",
            "Epoch: 300 | Train loss: 2.135 | Val loss: 2.169 | Time: 1.932 | ETA in sec 135.228\n",
            "Epoch: 310 | Train loss: 2.138 | Val loss: 2.206 | Time: 1.900 | ETA in sec 131.073\n",
            "Epoch: 320 | Train loss: 2.116 | Val loss: 2.190 | Time: 1.913 | ETA in sec 130.111\n",
            "Epoch: 330 | Train loss: 2.091 | Val loss: 2.163 | Time: 2.864 | ETA in sec 191.900\n",
            "Epoch: 340 | Train loss: 2.090 | Val loss: 2.184 | Time: 1.959 | ETA in sec 129.314\n",
            "Epoch: 350 | Train loss: 2.092 | Val loss: 2.179 | Time: 1.927 | ETA in sec 125.242\n",
            "Epoch: 360 | Train loss: 2.034 | Val loss: 2.179 | Time: 1.942 | ETA in sec 124.267\n",
            "Epoch: 370 | Train loss: 2.085 | Val loss: 2.184 | Time: 2.636 | ETA in sec 166.084\n",
            "Epoch: 380 | Train loss: 2.073 | Val loss: 2.143 | Time: 1.947 | ETA in sec 120.698\n",
            "Epoch: 390 | Train loss: 2.093 | Val loss: 2.193 | Time: 1.947 | ETA in sec 118.793\n",
            "Epoch: 400 | Train loss: 2.058 | Val loss: 2.140 | Time: 2.030 | ETA in sec 121.812\n",
            "Epoch: 410 | Train loss: 2.045 | Val loss: 2.116 | Time: 2.331 | ETA in sec 137.553\n",
            "Epoch: 420 | Train loss: 2.052 | Val loss: 2.130 | Time: 1.904 | ETA in sec 110.457\n",
            "Epoch: 430 | Train loss: 2.083 | Val loss: 2.099 | Time: 1.909 | ETA in sec 108.796\n",
            "Epoch: 440 | Train loss: 2.042 | Val loss: 2.096 | Time: 2.219 | ETA in sec 124.257\n",
            "Epoch: 450 | Train loss: 2.060 | Val loss: 2.098 | Time: 2.089 | ETA in sec 114.919\n",
            "Epoch: 460 | Train loss: 2.056 | Val loss: 2.085 | Time: 2.220 | ETA in sec 119.886\n",
            "Epoch: 470 | Train loss: 2.002 | Val loss: 2.067 | Time: 1.953 | ETA in sec 103.504\n",
            "Epoch: 480 | Train loss: 2.033 | Val loss: 2.087 | Time: 2.609 | ETA in sec 135.683\n",
            "Epoch: 490 | Train loss: 2.057 | Val loss: 2.099 | Time: 1.891 | ETA in sec 96.440\n",
            "Epoch: 500 | Train loss: 2.058 | Val loss: 2.081 | Time: 1.951 | ETA in sec 97.567\n",
            "Epoch: 510 | Train loss: 2.006 | Val loss: 2.109 | Time: 1.920 | ETA in sec 94.099\n",
            "Epoch: 520 | Train loss: 2.069 | Val loss: 2.116 | Time: 3.093 | ETA in sec 148.454\n",
            "Epoch: 530 | Train loss: 2.043 | Val loss: 2.097 | Time: 1.966 | ETA in sec 92.398\n",
            "Epoch: 540 | Train loss: 1.999 | Val loss: 2.074 | Time: 1.936 | ETA in sec 89.077\n",
            "Epoch: 550 | Train loss: 1.983 | Val loss: 2.092 | Time: 2.166 | ETA in sec 97.489\n",
            "Epoch: 560 | Train loss: 2.029 | Val loss: 2.118 | Time: 2.201 | ETA in sec 96.834\n",
            "Epoch: 570 | Train loss: 2.000 | Val loss: 2.107 | Time: 1.932 | ETA in sec 83.064\n",
            "Epoch: 580 | Train loss: 1.965 | Val loss: 2.067 | Time: 1.916 | ETA in sec 80.452\n",
            "Epoch: 590 | Train loss: 2.061 | Val loss: 2.036 | Time: 2.341 | ETA in sec 95.994\n",
            "Epoch: 600 | Train loss: 1.984 | Val loss: 2.067 | Time: 1.937 | ETA in sec 77.464\n",
            "Epoch: 610 | Train loss: 1.954 | Val loss: 2.043 | Time: 1.912 | ETA in sec 74.573\n",
            "Epoch: 620 | Train loss: 2.000 | Val loss: 2.019 | Time: 1.929 | ETA in sec 73.309\n",
            "Epoch: 630 | Train loss: 1.949 | Val loss: 2.066 | Time: 2.643 | ETA in sec 97.788\n",
            "Epoch: 640 | Train loss: 1.955 | Val loss: 2.074 | Time: 2.575 | ETA in sec 92.686\n",
            "Epoch: 650 | Train loss: 1.986 | Val loss: 2.073 | Time: 2.917 | ETA in sec 102.079\n",
            "Epoch: 660 | Train loss: 1.937 | Val loss: 2.051 | Time: 2.339 | ETA in sec 79.512\n",
            "Epoch: 670 | Train loss: 1.987 | Val loss: 2.042 | Time: 2.159 | ETA in sec 71.251\n",
            "Epoch: 680 | Train loss: 1.983 | Val loss: 2.022 | Time: 2.547 | ETA in sec 81.514\n",
            "Epoch: 690 | Train loss: 1.942 | Val loss: 2.081 | Time: 2.036 | ETA in sec 63.126\n",
            "Epoch: 700 | Train loss: 1.957 | Val loss: 2.021 | Time: 2.545 | ETA in sec 76.341\n",
            "Epoch: 710 | Train loss: 1.973 | Val loss: 2.053 | Time: 2.889 | ETA in sec 83.774\n",
            "Epoch: 720 | Train loss: 1.970 | Val loss: 2.084 | Time: 1.913 | ETA in sec 53.575\n",
            "Epoch: 730 | Train loss: 1.934 | Val loss: 2.040 | Time: 3.157 | ETA in sec 85.229\n",
            "Epoch: 740 | Train loss: 1.927 | Val loss: 2.103 | Time: 2.124 | ETA in sec 55.219\n",
            "Epoch: 750 | Train loss: 1.954 | Val loss: 2.069 | Time: 2.487 | ETA in sec 62.182\n",
            "Epoch: 760 | Train loss: 1.909 | Val loss: 1.997 | Time: 2.181 | ETA in sec 52.347\n",
            "Epoch: 770 | Train loss: 1.962 | Val loss: 2.049 | Time: 2.208 | ETA in sec 50.780\n",
            "Epoch: 780 | Train loss: 1.944 | Val loss: 2.040 | Time: 1.965 | ETA in sec 43.237\n",
            "Epoch: 790 | Train loss: 1.963 | Val loss: 2.085 | Time: 1.924 | ETA in sec 40.402\n",
            "Epoch: 800 | Train loss: 1.973 | Val loss: 2.096 | Time: 2.455 | ETA in sec 49.107\n",
            "Epoch: 810 | Train loss: 1.970 | Val loss: 2.035 | Time: 1.889 | ETA in sec 35.900\n",
            "Epoch: 820 | Train loss: 1.933 | Val loss: 1.994 | Time: 1.910 | ETA in sec 34.378\n",
            "Epoch: 830 | Train loss: 1.972 | Val loss: 2.017 | Time: 2.147 | ETA in sec 36.500\n",
            "Epoch: 840 | Train loss: 1.960 | Val loss: 2.022 | Time: 3.180 | ETA in sec 50.879\n",
            "Epoch: 850 | Train loss: 1.926 | Val loss: 2.043 | Time: 1.916 | ETA in sec 28.739\n",
            "Epoch: 860 | Train loss: 1.924 | Val loss: 2.080 | Time: 1.995 | ETA in sec 27.935\n",
            "Epoch: 870 | Train loss: 1.937 | Val loss: 2.089 | Time: 2.151 | ETA in sec 27.969\n",
            "Epoch: 880 | Train loss: 1.964 | Val loss: 2.047 | Time: 2.309 | ETA in sec 27.713\n",
            "Epoch: 890 | Train loss: 1.946 | Val loss: 2.029 | Time: 1.885 | ETA in sec 20.739\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-666af6f84a41>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_logs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-50-7696d56d4e63>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, config, print_logs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;31m# Evaluate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m       \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-9dcaebe59409>\u001b[0m in \u001b[0;36mevaluate_loss\u001b[0;34m(model, config)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context_window\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;31m# Run the model and calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-c70a85c73ae2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Attention block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrope_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Linear block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-5edc1e2691ce>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Process each attention head and concatenate the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-5edc1e2691ce>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Process each attention head and concatenate the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-d4c29bdafbd3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_attn_weights)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Rotate Q and K using the RoPE matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Text"
      ],
      "metadata": {
        "id": "HaS9gN6glrXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using the model\n",
        "def generate(model, config=MASTER_CONFIG, num_texts=5, max_new_tokens=30):\n",
        "  # Initialize with zeros = '\\n'\n",
        "  idx = torch.zeros(num_texts, 1).long()\n",
        "\n",
        "  for _ in range(max_new_tokens):\n",
        "    # Call the model\n",
        "    logits = model(idx[:, -config[\"context_window\"]:])\n",
        "    # Get the newest logits: [all batches, last time step, all logits]\n",
        "    last_time_step_logits = logits[:, -1, :]\n",
        "    # Softmax probabilities\n",
        "    p = F.softmax(last_time_step_logits, dim=-1)\n",
        "    # Get the next token from the probabilities\n",
        "    idx_next = torch.multinomial(p, num_samples=1)\n",
        "    # Append the token\n",
        "    idx = torch.cat([idx, idx_next], dim=-1)\n",
        "\n",
        "  # Return the decoded tokens\n",
        "  return [decode(x) for x in idx.tolist()]"
      ],
      "metadata": {
        "id": "wevzDU4eltrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model)"
      ],
      "metadata": {
        "id": "TYExYXQXlu1J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}