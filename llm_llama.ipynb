{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "id": "m3m5msf5lEx2"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ydXrYfUA3zJ",
        "outputId": "07f19a45-cc08-489c-d076-951744c0b96a"
      },
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "MASTER_CONFIG = {\n",
        "    \"vocab_size\": 65,       # Number of unique characters in the dataset\n",
        "    \"batch_size\": 32,       # Number of batches\n",
        "    \"context_window\": 16,   # Number of characters in a batch\n",
        "    \"d_model\": 128,         # Dimension of linear layers\n",
        "    \"epochs\": 10_000,       # Number of training epochs\n",
        "    \"log_interval\": 100,    # Frequency of logging the loss in epochs\n",
        "    \"n_heads\": 8,           # Number of attention heads\n",
        "    \"n_layers\": 4           # Number of Llama layers\n",
        "}"
      ],
      "metadata": {
        "id": "VBCJ9JTTlJbi"
      },
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "hDzje7UalLRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tiny Shakespeare\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "file_name = \"tinyshakespeare.txt\"\n",
        "urllib.request.urlretrieve(url, file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n4ItxJ5lMGV",
        "outputId": "23df0e86-9780-4a23-e75a-32c08a51838b"
      },
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tinyshakespeare.txt', <http.client.HTTPMessage at 0x7d6098398e20>)"
            ]
          },
          "metadata": {},
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocab list of unique chars\n",
        "lines = open(\"tinyshakespeare.txt\", 'r').read()\n",
        "vocab = sorted(list(set(lines)))"
      ],
      "metadata": {
        "id": "Qbb_QEirlPHT"
      },
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Int to string\n",
        "itos = {i: ch for i, ch in enumerate(vocab)}\n",
        "\n",
        "# String to int\n",
        "stoi = {ch: i for i, ch in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "fVWOD7M2lQqx"
      },
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding and decoding\n",
        "def encode(s):\n",
        "    return [stoi[ch] for ch in s]\n",
        "\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "7xxeMFullSah"
      },
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataset into a tensor\n",
        "dataset = torch.tensor(encode(lines), dtype=torch.int8, device=device)"
      ],
      "metadata": {
        "id": "GDwuDSKDlWnf"
      },
      "execution_count": 314,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into train/val/test batches\n",
        "def get_batches(data, split, batch_size, context_window, config=MASTER_CONFIG):\n",
        "  # Train/val/test split = 0.8/0.1/0.1\n",
        "  train = data[:int(0.8 * len(data))]\n",
        "  val = data[int(0.8 * len(data)):int(0.9 * len(data))]\n",
        "  test = data[int(0.9 * len(data)):]\n",
        "\n",
        "  # Determine which batch to use\n",
        "  batch_data = train\n",
        "  if split == \"val\":\n",
        "    batch_data = val\n",
        "  elif split == \"test\":\n",
        "    batch_data = test\n",
        "\n",
        "  # batch_size number of random starting points in the data\n",
        "  ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
        "\n",
        "  # Input and target sequences\n",
        "  x = torch.stack([batch_data[i:i + context_window] for i in ix]).long()\n",
        "  y = torch.stack([batch_data[i + 1:i + context_window + 1] for i in ix]).long()\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "V9-olP70lYU3"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "id": "jSn1oXjVla5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes the mean loss for 10 batches for train/val\n",
        "@torch.no_grad()\n",
        "def evaluate_loss(model, config=MASTER_CONFIG):\n",
        "  out = {}\n",
        "\n",
        "  # Set the model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Iterate through train and val splits\n",
        "  for split in [\"train\", \"val\"]:\n",
        "    losses = []\n",
        "\n",
        "    # Get 10 sample batches\n",
        "    for _ in range(10):\n",
        "      # Input and target sequences\n",
        "      xb, yb = get_batches(dataset, split, config[\"batch_size\"], config[\"context_window\"])\n",
        "      # Run the model and calculate the loss\n",
        "      _, loss = model(xb, yb)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "    out[split] = np.mean(losses)\n",
        "\n",
        "  # Set the model to train mode\n",
        "  model.train()\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "Z0VujPjxlbdp"
      },
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama Model"
      ],
      "metadata": {
        "id": "dfuzrPk6ld6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, layer_shape, eps=1e-8, bias=False):\n",
        "    super(RMSNorm, self).__init__()\n",
        "\n",
        "    # Register a learnable parameter \"scale\" as a part of the module\n",
        "    self.register_parameter(\"scale\", nn.Parameter(torch.ones(layer_shape)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (batch, seq_len, d_model)\n",
        "\n",
        "    # Calculate Frobenius norm: RMS = 1/sqrt(N) * Frobenius norm\n",
        "    ff_rms = torch.linalg.norm(x, dim=(1, 2)) * x[0].numel() ** -0.5\n",
        "\n",
        "    # Normalize x with respect to RMS\n",
        "    raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "    # Scale the normalized tensor using the learnable parameter \"scale\"\n",
        "    return self.scale[:x.shape[1], :].unsqueeze(0) * raw"
      ],
      "metadata": {
        "id": "LHEVqscNl0Hq"
      },
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rotary_matrix(context_window, embedding_dim):\n",
        "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False, device=device)\n",
        "\n",
        "    # Loop through each position in the context window\n",
        "    for position in range(context_window):\n",
        "      # Loop through each dimension in the embedding\n",
        "      for i in range(embedding_dim // 2):\n",
        "        # Calculate the rotation angle based on the position and embedding dimension\n",
        "        theta = 10_000 ** (-2 * (i - 1) / embedding_dim)\n",
        "        # Calculate the rotated matrix elements\n",
        "        m_theta = position * theta\n",
        "        R[position, 2 * i, 2 * i] = np.cos(m_theta)\n",
        "        R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)\n",
        "        R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)\n",
        "        R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)\n",
        "\n",
        "    return R\n",
        "\n",
        "class RoPEMaskedAttentionHead(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    # Linear transformation for query\n",
        "    self.w_q = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "    # Linear transformation for key\n",
        "    self.w_k = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "    # Linear transformation for value\n",
        "    self.w_v = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "    # Obtain rotary matrix for positional embeddings\n",
        "    self.R = get_rotary_matrix(config['context_window'], config['d_model'])\n",
        "\n",
        "  def forward(self, x, return_attn_weights=False):\n",
        "    # x: (batch, seq_len, d_model)\n",
        "    b, m, d = x.shape\n",
        "\n",
        "    # Linear transformations for Q, K, and V\n",
        "    q = self.w_q(x)\n",
        "    k = self.w_k(x)\n",
        "    v = self.w_v(x)\n",
        "\n",
        "    # Rotate Q and K using the RoPE matrix\n",
        "    q_rotated = (torch.bmm(q.transpose(0, 1), self.R[:m])).transpose(0, 1)\n",
        "    k_rotated = (torch.bmm(k.transpose(0, 1), self.R[:m])).transpose(0, 1)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    activations = F.scaled_dot_product_attention(q_rotated, k_rotated, v, dropout_p=0.1, is_causal=True)\n",
        "\n",
        "    # Return the attention weights\n",
        "    if return_attn_weights:\n",
        "      # Create causal attention mask\n",
        "      attn_mask = torch.tril(torch.ones((m, m)), diagonal=0)\n",
        "      # Calculate attention weights and add causal mask\n",
        "      attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1, 2)) / np.sqrt(d) + attn_mask\n",
        "      attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "      return activations, attn_weights\n",
        "\n",
        "    return activations"
      ],
      "metadata": {
        "id": "FOza2XT3rmkV"
      },
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoPEMaskedMultiheadAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    # RoPEMaskedAttentionHead instances as attention heads\n",
        "    self.heads = nn.ModuleList([RoPEMaskedAttentionHead(config) for _ in range(config[\"n_heads\"])])\n",
        "    # Linear layer after concatenating the heads\n",
        "    self.linear = nn.Linear(config[\"n_heads\"] * config[\"d_model\"], config[\"d_model\"])\n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (batch, seq_len, d_model)\n",
        "\n",
        "    # Process each attention head and concatenate the results\n",
        "    heads = [h(x) for h in self.heads]\n",
        "    x = torch.cat(heads, dim=-1)\n",
        "    # Linear layer\n",
        "    x = self.linear(x)\n",
        "    # Dropout layer\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "tw7ob6kHyb3C"
      },
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwiGLU(nn.Module):\n",
        "  def __init__(self, size):\n",
        "    super().__init__()\n",
        "\n",
        "    # Linear transformation for the gate mechanism\n",
        "    self.linear_gate = nn.Linear(size, size)\n",
        "    # Linear transformation for the main branch\n",
        "    self.linear = nn.Linear(size, size)\n",
        "    # Randomly init the beta parameter\n",
        "    self.beta = torch.randn(1, requires_grad=True)\n",
        "\n",
        "    # Register beta as a learnable parameter\n",
        "    self.beta = nn.Parameter(torch.ones(1))\n",
        "    self.register_parameter(\"beta\", self.beta)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Swish-Gated Linear Unit computation\n",
        "    swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))\n",
        "    # Element-wise multiplication between the gate and main branch\n",
        "    out = swish_gate * self.linear(x)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "9jRyJHtD2ont"
      },
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LlamaBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    # RMSNorm layer for pre-normalization\n",
        "    self.rms = RMSNorm((config[\"context_window\"], config[\"d_model\"]))\n",
        "    # RoPEMaskedMultiheadAttention layer\n",
        "    self.attention = RoPEMaskedMultiheadAttention(config)\n",
        "    # Feedforward layer with SwiGLU activation\n",
        "    self.feedforward = nn.Sequential(\n",
        "        nn.Linear(config[\"d_model\"], config[\"d_model\"]),\n",
        "        SwiGLU(config['d_model'])\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Attention block\n",
        "    x = self.rms(x)\n",
        "    x = x + self.attention(x)\n",
        "\n",
        "    # Linear block\n",
        "    x = self.rms(x)\n",
        "    x = x + self.feedforward(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "NnZ99ikd8IPQ"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Llama(nn.Module):\n",
        "  def __init__(self, config=MASTER_CONFIG):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    # Embedding layer to convert character indices to vectors\n",
        "    self.embedding = nn.Embedding(config[\"vocab_size\"], config[\"d_model\"])\n",
        "    # Sequential block of n_layers LlamaBlocks\n",
        "    self.llama_blocks = nn.Sequential(OrderedDict([(f\"llama_{i}\", LlamaBlock(config)) for i in range(config['n_layers'])]))\n",
        "    # Feedforward network for final output\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(config['d_model'], config['d_model']),\n",
        "        SwiGLU(config['d_model']),\n",
        "        nn.Linear(config['d_model'], config['vocab_size']),\n",
        "    )\n",
        "\n",
        "    print(\"# of params:\", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Embedding layer converts character indices to vectors\n",
        "    x = self.embedding(idx)\n",
        "\n",
        "    # Llama blocks\n",
        "    x = self.llama_blocks(x)\n",
        "    # Final feedforward network for output logits\n",
        "    logits = self.ffn(x)\n",
        "\n",
        "    # If there are targets, calculate and return the cross entropy loss\n",
        "    if targets is not None:\n",
        "      loss = F.cross_entropy(logits.view(-1, self.config[\"vocab_size\"]), targets.view(-1))\n",
        "      return logits, loss\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "Bisbawb6lfgd"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(model, optimizer, scheduler=None, config=MASTER_CONFIG, print_logs=False):\n",
        "  losses = []\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Iterate through epochs\n",
        "  for epoch in range(config[\"epochs\"]):\n",
        "    # Zero out gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Get training batches\n",
        "    xs, ys = get_batches(dataset, \"train\", config[\"batch_size\"], config[\"context_window\"])\n",
        "    # Run the model and calculate the loss\n",
        "    logits, loss = model(xs, targets=ys)\n",
        "    # Backpropagate the loss\n",
        "    loss.backward()\n",
        "    # Step the optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    # If there's a learning rate scheduler, adjust the learning rate\n",
        "    if scheduler:\n",
        "      scheduler.step()\n",
        "\n",
        "    # Log progress\n",
        "    if epoch % config[\"log_interval\"] == 0:\n",
        "      # Calculate batch time\n",
        "      batch_time = time.time() - start_time\n",
        "\n",
        "      # Evaluate loss\n",
        "      x = evaluate_loss(model)\n",
        "      losses += [x]\n",
        "\n",
        "      # Print progress\n",
        "      if print_logs:\n",
        "        print(f\"Epoch: {epoch} | Train loss: {x['train']:0.3f} | Val loss: {x['val']:0.3f} | Batch time: {batch_time:0.3f} | ETA (sec): {batch_time * (config['epochs'] - epoch) / config['log_interval']:0.3f}\")\n",
        "\n",
        "      # Reset timer\n",
        "      start_time = time.time()\n",
        "\n",
        "      # Print learning rate\n",
        "      if scheduler:\n",
        "        print(\"Learning rate:\", scheduler.get_last_lr()[0])\n",
        "\n",
        "  print(\"Val loss:\", losses[-1][\"val\"])\n",
        "\n",
        "  return pd.DataFrame(losses).plot()"
      ],
      "metadata": {
        "id": "AoLKTPIEljau"
      },
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinit model\n",
        "model = Llama(MASTER_CONFIG).to(device)\n",
        "# Adam optimizer with specific hyperparameters\n",
        "llama_optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    betas=(0.9, 0.95),\n",
        "    weight_decay=0.1,\n",
        "    eps=1e-9,\n",
        "    lr=1e-3\n",
        ")\n",
        "llama_optimizer = torch.optim.AdamW(model.parameters())\n",
        "# Cosine Annealing learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(llama_optimizer, 300, eta_min=1e-5)\n",
        "scheduler = None\n",
        "\n",
        "# Train model\n",
        "train(model, llama_optimizer, scheduler=scheduler, print_logs=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qcfSmKcbll9v",
        "outputId": "37796ffc-f6f0-4df2-e0ae-7ee64d593080"
      },
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of params: 2370246\n",
            "Epoch: 0 | Train loss: 4.169 | Val loss: 4.169 | Batch time: 0.064 | ETA (sec): 6.370\n",
            "Learning rate: 0.0009999728588359286\n",
            "Epoch: 100 | Train loss: 4.087 | Val loss: 4.087 | Batch time: 5.688 | ETA (sec): 563.119\n",
            "Learning rate: 0.0007479973580394943\n",
            "Epoch: 200 | Train loss: 4.052 | Val loss: 4.048 | Batch time: 6.822 | ETA (sec): 668.595\n",
            "Learning rate: 0.0002530244992035658\n",
            "Epoch: 300 | Train loss: 4.044 | Val loss: 4.046 | Batch time: 5.657 | ETA (sec): 548.693\n",
            "Learning rate: 1.002714116407149e-05\n",
            "Epoch: 400 | Train loss: 4.042 | Val loss: 4.043 | Batch time: 6.521 | ETA (sec): 626.033\n",
            "Learning rate: 0.00026200264196050104\n",
            "Epoch: 500 | Train loss: 4.019 | Val loss: 4.016 | Batch time: 5.607 | ETA (sec): 532.711\n",
            "Learning rate: 0.0007569755007964201\n",
            "Epoch: 600 | Train loss: 3.984 | Val loss: 3.982 | Batch time: 6.457 | ETA (sec): 606.972\n",
            "Learning rate: 0.000999972858835911\n",
            "Epoch: 700 | Train loss: 3.956 | Val loss: 3.966 | Batch time: 5.684 | ETA (sec): 528.586\n",
            "Learning rate: 0.0007479973580394816\n",
            "Epoch: 800 | Train loss: 3.948 | Val loss: 3.957 | Batch time: 6.365 | ETA (sec): 585.537\n",
            "Learning rate: 0.0002530244992035621\n",
            "Epoch: 900 | Train loss: 3.949 | Val loss: 3.956 | Batch time: 5.601 | ETA (sec): 509.683\n",
            "Learning rate: 1.002714116407149e-05\n",
            "Epoch: 1000 | Train loss: 3.943 | Val loss: 3.949 | Batch time: 6.800 | ETA (sec): 612.027\n",
            "Learning rate: 0.00026200264196050234\n",
            "Epoch: 1100 | Train loss: 3.931 | Val loss: 3.934 | Batch time: 5.670 | ETA (sec): 504.589\n",
            "Learning rate: 0.0007569755007964277\n",
            "Epoch: 1200 | Train loss: 3.923 | Val loss: 3.918 | Batch time: 6.255 | ETA (sec): 550.435\n",
            "Learning rate: 0.0009999728588359208\n",
            "Epoch: 1300 | Train loss: 3.904 | Val loss: 3.903 | Batch time: 6.769 | ETA (sec): 588.887\n",
            "Learning rate: 0.0007479973580394889\n",
            "Epoch: 1400 | Train loss: 3.892 | Val loss: 3.903 | Batch time: 5.759 | ETA (sec): 495.311\n",
            "Learning rate: 0.00025302449920356425\n",
            "Epoch: 1500 | Train loss: 3.894 | Val loss: 3.898 | Batch time: 6.396 | ETA (sec): 543.671\n",
            "Learning rate: 1.002714116407149e-05\n",
            "Epoch: 1600 | Train loss: 3.890 | Val loss: 3.900 | Batch time: 5.683 | ETA (sec): 477.377\n",
            "Learning rate: 0.00026200264196050077\n",
            "Epoch: 1700 | Train loss: 3.885 | Val loss: 3.892 | Batch time: 6.558 | ETA (sec): 544.329\n",
            "Learning rate: 0.0007569755007964233\n",
            "Epoch: 1800 | Train loss: 3.879 | Val loss: 3.887 | Batch time: 5.717 | ETA (sec): 468.762\n",
            "Learning rate: 0.0009999728588359154\n",
            "Epoch: 1900 | Train loss: 3.875 | Val loss: 3.878 | Batch time: 6.506 | ETA (sec): 527.012\n",
            "Learning rate: 0.0007479973580394858\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-324-ef5f2fbab858>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllama_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_logs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-323-c615d4b0d311>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, config, print_logs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Backpropagate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Step the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model, 'llama_model.pth')\n",
        "# Save the model params\n",
        "torch.save(model.state_dict(), 'llama_model_params.pth')"
      ],
      "metadata": {
        "id": "b-hKUInB_39U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Text"
      ],
      "metadata": {
        "id": "HaS9gN6glrXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using the model\n",
        "def generate(model, config=MASTER_CONFIG, num_texts=5, max_new_tokens=30):\n",
        "  # Initialize with zeros = '\\n'\n",
        "  idx = torch.zeros(num_texts, 1, device=device).long()\n",
        "\n",
        "  for _ in range(max_new_tokens):\n",
        "    # Call the model\n",
        "    logits = model(idx[:, -config[\"context_window\"]:])\n",
        "    # Get the newest logits: [all batches, last time step, all logits]\n",
        "    last_time_step_logits = logits[:, -1, :]\n",
        "    # Softmax probabilities\n",
        "    p = F.softmax(last_time_step_logits, dim=-1)\n",
        "    # Get the next token from the probabilities\n",
        "    idx_next = torch.multinomial(p, num_samples=1)\n",
        "    # Append the token\n",
        "    idx = torch.cat([idx, idx_next], dim=-1)\n",
        "\n",
        "  # Return the decoded tokens\n",
        "  return [decode(x) for x in idx.tolist()]"
      ],
      "metadata": {
        "id": "wevzDU4eltrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, num_texts=1, max_new_tokens=500)"
      ],
      "metadata": {
        "id": "TYExYXQXlu1J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}