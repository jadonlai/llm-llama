{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "QLD5lPCN3FNE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "MASTER_CONFIG = {\n",
        "    \"batch_size\": 8,      # Number of batches\n",
        "    \"context_window\": 16  # Number of characters in a batch\n",
        "}"
      ],
      "metadata": {
        "id": "T9tpAmIM3OhH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "aK3N5aGI2_gd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "GokVAEtp25QP",
        "outputId": "f5ce71c9-8641-4f44-9564-b46870835ee8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tinyshakespeare.txt', <http.client.HTTPMessage at 0x7eab0accaf50>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Tiny Shakespeare\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "file_name = \"tinyshakespeare.txt\"\n",
        "urllib.request.urlretrieve(url, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocab list of unique chars\n",
        "lines = open(\"tinyshakespeare.txt\", 'r').read()\n",
        "vocab = sorted(list(set(lines)))\n",
        "\n",
        "print(\"First 10 chars of vocab list:\", vocab[:10])\n",
        "print(\"Vocab size:\", len(vocab))"
      ],
      "metadata": {
        "id": "Mv2bfoLd3dPj",
        "outputId": "d1b24fa6-0650-4af7-f25b-fb7954d02d25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 chars of vocab list: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3']\n",
            "Vocab size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Int to string\n",
        "itos = {i: ch for i, ch in enumerate(vocab)}\n",
        "\n",
        "# String to int\n",
        "stoi = {ch: i for i, ch in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "0cxgUCV_4EtW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding and decoding\n",
        "def encode(s):\n",
        "    return [stoi[ch] for ch in s]\n",
        "\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l])\n",
        "\n",
        "# Example\n",
        "print(encode(\"hello world\"))\n",
        "print(decode(encode(\"hello world\")))"
      ],
      "metadata": {
        "id": "FpZia87w4QYA",
        "outputId": "1066379c-e7eb-4ad1-e434-cb221c226950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataset into a tensor\n",
        "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
        "\n",
        "# Approximately 1 million tokens (chars)\n",
        "print(dataset.shape)"
      ],
      "metadata": {
        "id": "vNQ0fUlR4e1u",
        "outputId": "a8cef498-009b-450e-d4ba-6279fa887587",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into train/val/test batches\n",
        "def get_batches(data, split, batch_size, context_window, config=MASTER_CONFIG):\n",
        "  # Train/val/test split = 0.8/0.1/0.1\n",
        "  train = data[:int(0.8 * len(data))]\n",
        "  val = data[int(0.8 * len(data)):int(0.9 * len(data))]\n",
        "  test = data[int(0.9 * len(data)):]\n",
        "\n",
        "  # Determine which batch to use\n",
        "  batch_data = train\n",
        "  if split == \"val\":\n",
        "    batch_data = val\n",
        "  elif split == \"test\":\n",
        "    batch_data = test\n",
        "\n",
        "  # batch_size number of random starting points in the data\n",
        "  ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
        "\n",
        "  # Input and target sequences\n",
        "  x = torch.stack([batch_data[i:i + context_window] for i in ix]).long()\n",
        "  y = torch.stack([batch_data[i + 1:i + context_window + 1] for i in ix]).long()\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "bSARofwL4z31"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain batches for training\n",
        "xs, ys = get_batches(dataset, \"train\", MASTER_CONFIG[\"batch_size\"], MASTER_CONFIG[\"context_window\"])\n",
        "\n",
        "# Decode batches\n",
        "decoded_samples = [(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))]\n",
        "\n",
        "# (sample, target)\n",
        "print(decoded_samples)"
      ],
      "metadata": {
        "id": "zLCjwJTQ639k",
        "outputId": "760418b8-36ee-44bb-bb03-b87db4bc21fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('desires,\\nI am fr', 'esires,\\nI am fri'), ('r heirs, God, if', ' heirs, God, if '), ('hurch,\\nShall hap', 'urch,\\nShall happ'), ('hat make her goo', 'at make her good'), ('nown to the camp', 'own to the camp,'), ('And then awake a', 'nd then awake as'), ('onour\\nThan one o', 'nour\\nThan one on'), ('\\nNot fearing out', 'Not fearing outw')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "id": "eTN_5v-R8z8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes the mean loss for 10 batches for train/val\n",
        "@torch.no_grad()\n",
        "def evaluate_loss(model, config=MASTER_CONFIG):\n",
        "  out = {}\n",
        "\n",
        "  # Set the model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Iterate through train and val splits\n",
        "  for split in [\"train\", \"val\"]:\n",
        "    losses = []\n",
        "\n",
        "    # Get 10 sample batches\n",
        "    for _ in range(10):\n",
        "      # Input and target sequences\n",
        "      xb, yb = get_batches(dataset, split, config[\"batch_size\"], config[\"context_window\"])\n",
        "      # Run the model and calculate the loss\n",
        "      _, loss = model(xb, yb)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "    out[split] = np.mean(losses)\n",
        "\n",
        "  # Set the model to train mode\n",
        "  model.train()\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "GnsmkYd581G-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base NN"
      ],
      "metadata": {
        "id": "WtCxVy_N-_TA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lVMDgNc---26"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}