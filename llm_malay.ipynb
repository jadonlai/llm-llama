{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWcsyrn6cqe0",
        "outputId": "cc14b768-7eff-4050-908d-3a756547c0c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "!pip install datasets tokenizers\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dirs = [\"./malaygpt\", \"./tokenizer_en\", \"./tokenizer_my\"]\n",
        "for dir in dirs:\n",
        "  if os.path.exists(dir):\n",
        "    continue\n",
        "  os.mkdir(dir)"
      ],
      "metadata": {
        "id": "YUFT0-ZXvKtx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpgcofHKc1lv",
        "outputId": "6198310a-1be5-4cff-8adf-4a9f635a9c91"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "ccNZOJ8SczVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# English/Malay pairs from HuggingFace\n",
        "train_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ms\", split='train')\n",
        "validation_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ms\", split='validation')\n",
        "\n",
        "# Limit the amount of data for training purposes\n",
        "raw_train_dataset, rt_to_skip = random_split(train_dataset, [1500, len(train_dataset) - 1500])\n",
        "raw_validation_dataset, vt_to_skip = random_split(validation_dataset, [50, len(validation_dataset) - 50])"
      ],
      "metadata": {
        "id": "YbPjMyoVc0Vj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "AGssMfD0dqOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a generator list from a dataset of the given language\n",
        "def get_ds_iterator(raw_train_dataset, lang):\n",
        "  for data in raw_train_dataset:\n",
        "    yield data[\"translation\"][lang]\n",
        "\n",
        "# Create English source tokenizer\n",
        "tokenizer_en = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer_en = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "# Pre-tokenizer to split input into words\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "tokenizer_en.train_from_iterator(get_ds_iterator(raw_train_dataset, \"en\"), trainer=trainer_en)\n",
        "tokenizer_en.save(\"./tokenizer_en/tokenizer_en.json\")\n",
        "\n",
        "# Create Malay source tokenizer\n",
        "tokenizer_my = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer_my = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "# Pre-tokenizer to split input into words\n",
        "tokenizer_my.pre_tokenizer = Whitespace()\n",
        "tokenizer_my.train_from_iterator(get_ds_iterator(raw_train_dataset, \"ms\"), trainer=trainer_my)\n",
        "tokenizer_my.save(\"./tokenizer_my/tokenizer_my.json\")"
      ],
      "metadata": {
        "id": "S90CfWtFdwLd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve tokenizers we made\n",
        "tokenizer_en = Tokenizer.from_file(\"./tokenizer_en/tokenizer_en.json\")\n",
        "tokenizer_my = Tokenizer.from_file(\"./tokenizer_my/tokenizer_my.json\")\n",
        "\n",
        "# Get the vocab sizes\n",
        "source_vocab_size = tokenizer_en.get_vocab_size()\n",
        "target_vocab_size = tokenizer_my.get_vocab_size()"
      ],
      "metadata": {
        "id": "v_QXS5hYg77_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len_source = 0\n",
        "max_seq_len_target = 0\n",
        "\n",
        "# Calculate the max sequence length in the training dataset for source/target\n",
        "for data in raw_train_dataset:\n",
        "  enc_ids = tokenizer_en.encode(data[\"translation\"][\"en\"]).ids\n",
        "  dec_ids = tokenizer_my.encode(data[\"translation\"][\"ms\"]).ids\n",
        "  max_seq_len_source = max(max_seq_len_source, len(enc_ids))\n",
        "  max_seq_len_target = max(max_seq_len_target, len(dec_ids))\n",
        "\n",
        "print(\"Source vocab max sequence length:\", max_seq_len_source)\n",
        "print(\"Target vocab max sequence length:\", max_seq_len_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Voe9Sjhwsgl7",
        "outputId": "65fc33a5-c647-4094-a996-2cb6fa7aa215"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source vocab max sequence length: 87\n",
            "Target vocab max sequence length: 105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard max sequence length for training, with buffer for padding, the classification token, unknown tokens, separator tokens, etc.\n",
        "max_seq_len = 155"
      ],
      "metadata": {
        "id": "MshZzC_HtnDJ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Dataloader"
      ],
      "metadata": {
        "id": "ccMVO_M1zK6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Causal mask to hide future tokens\n",
        "def causal_mask(size):\n",
        "  # Square matrix with ones in the lower triangle: size x size\n",
        "  mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
        "  return mask == 0"
      ],
      "metadata": {
        "id": "3jeluc5ScAEt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode raw dataset to be processed by the model\n",
        "class EncodeDataset(Dataset):\n",
        "  def __init__(self, raw_dataset, max_seq_len):\n",
        "    super().__init__()\n",
        "    self.raw_dataset = raw_dataset\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.raw_dataset)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Fetch data (in both English and Malay) for the given index\n",
        "    raw_text = self.raw_dataset[index]\n",
        "\n",
        "    # Separate text into source and target\n",
        "    source_text = raw_text[\"translation\"][\"en\"]\n",
        "    target_text = raw_text[\"translation\"][\"ms\"]\n",
        "\n",
        "    # Encode text\n",
        "    source_text_encoded = tokenizer_en.encode(source_text).ids\n",
        "    target_text_encoded = tokenizer_my.encode(target_text).ids\n",
        "\n",
        "    # Convert CLS, SEP, and PAD to their vocab index id using the tokenizer\n",
        "    # Start of sentence token\n",
        "    CLS_ID = torch.tensor([tokenizer_my.token_to_id(\"[CLS]\")], dtype=torch.int64)\n",
        "    # End of sentence token\n",
        "    SEP_ID = torch.tensor([tokenizer_my.token_to_id(\"[SEP]\")], dtype=torch.int64)\n",
        "    # Padding token\n",
        "    PAD_ID = torch.tensor([tokenizer_my.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "    # Amount to pad the encoded text\n",
        "    num_source_padding = self.max_seq_len - len(source_text_encoded) - 2\n",
        "    num_target_padding = self.max_seq_len - len(target_text_encoded) - 1\n",
        "    encoder_padding = torch.tensor([PAD_ID] * num_source_padding, dtype=torch.int64)\n",
        "    decoder_padding = torch.tensor([PAD_ID] * num_target_padding, dtype=torch.int64)\n",
        "\n",
        "    # CLS + source encoding + SEP + padding\n",
        "    encoder_input = torch.cat([CLS_ID, torch.tensor(source_text_encoded, dtype=torch.int64), SEP_ID, encoder_padding], dim=0)\n",
        "    # CLS + target encoding + padding\n",
        "    decoder_input = torch.cat([CLS_ID, torch.tensor(target_text_encoded, dtype=torch.int64), decoder_padding], dim=0)\n",
        "\n",
        "    # target encoding + SEP + padding\n",
        "    target_label = torch.cat([torch.tensor(target_text_encoded, dtype=torch.int64), SEP_ID, decoder_padding], dim=0)\n",
        "\n",
        "    # Masks to ignore padding\n",
        "    encoder_mask = (encoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int()\n",
        "    # Apply causal mask to decoder mask, so that the decoder can't see future tokens when predicting the next token\n",
        "    decoder_mask = (decoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0))\n",
        "\n",
        "    return {\n",
        "        \"encoder_input\": encoder_input,\n",
        "        \"decoder_input\": decoder_input,\n",
        "        \"target_label\": target_label,\n",
        "        \"encoder_mask\": encoder_mask,\n",
        "        \"decoder_mask\": decoder_mask,\n",
        "        \"source_text\": source_text,\n",
        "        \"target_text\": target_text\n",
        "    }"
      ],
      "metadata": {
        "id": "05HQMYeJXGE7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create encoded datasets\n",
        "train_ds = EncodeDataset(raw_train_dataset, max_seq_len)\n",
        "val_ds = EncodeDataset(raw_validation_dataset, max_seq_len)\n",
        "\n",
        "# Create dataloaders to use in the model\n",
        "train_dataloader = DataLoader(train_ds, batch_size=5, shuffle=True)\n",
        "val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "KJ6AwuEXEcS3"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "encoder_input: Encoded source text with start and end of sentence tokens and padding\n",
        "decoder_input: Encoded target text with start of sentence token and padding\n",
        "target_label: Encoded target text with padding\n",
        "encoder_mask: Mask to ignore padding in the encoder input\n",
        "decoder_mask: (Causal) mask to ignore padding in the decoder input\n",
        "source_text: Original source text\n",
        "target_text: Original target text\n",
        "'''\n",
        "train_ds.__getitem__(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xfTHRbueAmi",
        "outputId": "91532035-8a30-4962-cdce-f8d054b6b0a2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'encoder_input': tensor([   2,   54,  220,  114, 1306,  109,    3,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
              " 'decoder_input': tensor([   2, 2196,  289,   40, 1970,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
              " 'target_label': tensor([2196,  289,   40, 1970,    3,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
              " 'encoder_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]],\n",
              "        dtype=torch.int32),\n",
              " 'decoder_mask': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
              "          [1, 1, 0,  ..., 0, 0, 0],\n",
              "          [1, 1, 1,  ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [1, 1, 1,  ..., 0, 0, 0],\n",
              "          [1, 1, 1,  ..., 0, 0, 0],\n",
              "          [1, 1, 1,  ..., 0, 0, 0]]], dtype=torch.int32),\n",
              " 'source_text': 'Spell Checker',\n",
              " 'target_text': 'Penyamak Ejaan'}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input Embedding and Positional Encoding"
      ],
      "metadata": {
        "id": "UD2iPLMjfo3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer with normalized embeddings\n",
        "class EmbeddingLayer(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    # Embedding layer to map token ids to embeddings (vocab_size x d_model)\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, input):\n",
        "    # Multiply embedding by the sqrt(d_model) to normalize the output\n",
        "    embedding_output = self.embedding(input) * math.sqrt(self.d_model)\n",
        "    return embedding_output"
      ],
      "metadata": {
        "id": "sVPnN5q5fqeL"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional encoding layer\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, max_seq_len: int, dropout_rate: float):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    # Init positional encodings, positions\n",
        "    pe = torch.zeros(max_seq_len, d_model)\n",
        "    pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    # 1 / (10000 ** (2 * i / d_model))\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "    # Apply div term to positional encodings, with sin/cos depending on even/odd dimensions\n",
        "    pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "    pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "\n",
        "    # Add batch dimension\n",
        "    # pe: 1 x seq_len x d_model\n",
        "    pe = pe.unsqueeze(0)\n",
        "    # Ensure that the positional encodings are a part of the model, but not trainable\n",
        "    self.register_buffer(\"pe\", pe)\n",
        "\n",
        "  def forward(self, input_embedding):\n",
        "    # input_embedding: batch_size x seq_len x d_model\n",
        "    input_embedding = input_embedding + (self.pe[:, :input_embedding.shape[1], :]).requires_grad_(False)\n",
        "    return self.dropout(input_embedding)"
      ],
      "metadata": {
        "id": "eTBhskyJhcQ-"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention"
      ],
      "metadata": {
        "id": "xbsFakYnswg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multihead attention block to get context\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model: int, num_heads: int, dropout_rate: float):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    # d_model must be divisible by the number of heads\n",
        "    assert d_model % num_heads == 0\n",
        "\n",
        "    # Dimension of each self attention head\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    # Init weight matrices\n",
        "    self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "  def forward(self, q, k, v, encoder_mask):\n",
        "    # q, k, v: batch_size x seq_len x d_model\n",
        "\n",
        "    # Multiply input embeddings by weights\n",
        "    query = self.W_q(q)\n",
        "    key = self.W_k(k)\n",
        "    value = self.W_v(v)\n",
        "\n",
        "    # Divide query, key, and value into the number of heads\n",
        "    # query, key, value: batch_size x num_heads x seq_len x d_k\n",
        "    query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    # SELF ATTENTION BLOCK\n",
        "    # -------------------------\n",
        "\n",
        "    # Attention score based on the similarity between the query and key\n",
        "    # attention_score: batch_size x num_heads x seq_len x seq_len\n",
        "    attention_score = (query @ key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "    # Apply encoder/causal mask\n",
        "    if encoder_mask is not None:\n",
        "      attention_score.masked_fill_(encoder_mask == 0, -1e9)\n",
        "\n",
        "    # Apply softmax\n",
        "    attention_score = attention_score.softmax(dim=-1)\n",
        "\n",
        "    # Apply dropout\n",
        "    if self.dropout is not None:\n",
        "      attention_score = self.dropout(attention_score)\n",
        "\n",
        "    # Multiply attention score with the value\n",
        "    # attention_output: batch_size x num_heads x seq_len x d_k\n",
        "    attention_output = attention_score @ value\n",
        "\n",
        "    # -------------------------\n",
        "\n",
        "    # Concatenate all the output heads\n",
        "    # attention_output: batch_size x seq_len x d_model\n",
        "    attention_output = attention_output.transpose(1, 2).contiguous().view(attention_output.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "    # Multiply attention output by output weights\n",
        "    multihead_output = self.W_o(attention_output)\n",
        "\n",
        "    return multihead_output"
      ],
      "metadata": {
        "id": "KL3cSgHDswAJ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feedforward, Layer Normalization, and AddAndNorm"
      ],
      "metadata": {
        "id": "GJsZAsL9z7YP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Two linear layers, with dropout and ReLU activation\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model: int, d_ff: int, dropout_rate: float):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.layer_1 = nn.Linear(d_model, d_ff)\n",
        "    self.layer_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.layer_2(self.dropout(torch.relu(self.layer_1(input))))"
      ],
      "metadata": {
        "id": "yfilODjF0AEB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer normalization with scaling (gamma) and shifting (beta)\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, eps: float = 1e-5):\n",
        "    super().__init__()\n",
        "    # Epsilon is for divide-by-zero errors\n",
        "    self.eps = eps\n",
        "    # Extra learning params to scale and shift embedding values; same number of weights as d_model\n",
        "    self.gamma = nn.Parameter(torch.ones(512))\n",
        "    self.beta = nn.Parameter(torch.zeros(512))\n",
        "\n",
        "  def forward(self, input):\n",
        "    mean = input.mean(dim=-1, keepdim=True)\n",
        "    std = input.std(dim=-1, keepdim=True)\n",
        "    return self.gamma * (input - mean) / (std + self.eps) + self.beta"
      ],
      "metadata": {
        "id": "iOLDlOav0em-"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer normalization and skip connection\n",
        "class AddAndNorm(nn.Module):\n",
        "  def __init__(self, dropout_rate: float):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.layer_norm = LayerNorm()\n",
        "\n",
        "  def forward(self, input, sub_layer):\n",
        "    return input + self.dropout(sub_layer(self.layer_norm(input)))"
      ],
      "metadata": {
        "id": "W22oPP8F2Kfq"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Block and Encoder"
      ],
      "metadata": {
        "id": "5WkmqUqY36LR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multihead attention and feed forward blocks, with add-and-norm\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float) -> None:\n",
        "    super().__init__()\n",
        "    self.multihead_attention = multihead_attention\n",
        "    self.feed_forward = feed_forward\n",
        "    self.addnorm_1 = AddAndNorm(dropout_rate)\n",
        "    self.addnorm_2 = AddAndNorm(dropout_rate)\n",
        "\n",
        "  def forward(self, encoder_input, encoder_mask):\n",
        "    # Encoder input from skip connection and Multihead Attention block\n",
        "    encoder_input = self.addnorm_1(encoder_input, lambda encoder_input: self.multihead_attention(encoder_input, encoder_input, encoder_input, encoder_mask))\n",
        "    # Multihead Attention output from skip connection and Feed Forward block\n",
        "    encoder_input = self.addnorm_2(encoder_input, self.feed_forward)\n",
        "\n",
        "    return encoder_input"
      ],
      "metadata": {
        "id": "05xfP24BBnAx"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple encoder blocks and layer normalization\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, encoderblocklist: nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.encoderblocklist = encoderblocklist\n",
        "    self.layer_norm = LayerNorm()\n",
        "\n",
        "  def forward(self, encoder_input, encoder_mask):\n",
        "    # Loop input through all encoder blocks\n",
        "    for encoderblock in self.encoderblocklist:\n",
        "      encoder_input = encoderblock(encoder_input, encoder_mask)\n",
        "    # Normalize the final encoder block output\n",
        "    encoder_output = self.layer_norm(encoder_input)\n",
        "    return encoder_output"
      ],
      "metadata": {
        "id": "tghbeybhEYP4"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Block, Decoder, and Projection Layer"
      ],
      "metadata": {
        "id": "2dY7dD5kFP9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Masked multihead attention, cross multihead attention from encoder output, and feed forward blocks, with add-and-norm\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, masked_multihead_attention: MultiHeadAttention, cross_multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float) -> None:\n",
        "    super().__init__()\n",
        "    # Uses a causal mask\n",
        "    self.masked_multihead_attention = masked_multihead_attention\n",
        "    # Uses multihead attention from the output of the encoder\n",
        "    self.cross_multihead_attention = cross_multihead_attention\n",
        "    self.feed_forward = feed_forward\n",
        "    self.addnorm_1 = AddAndNorm(dropout_rate)\n",
        "    self.addnorm_2 = AddAndNorm(dropout_rate)\n",
        "    self.addnorm_3 = AddAndNorm(dropout_rate)\n",
        "\n",
        "  def forward(self, decoder_input, encoder_output, encoder_mask, decoder_mask):\n",
        "    # Decoder input from skip connection and Masked Multihead Attention block\n",
        "    decoder_input = self.addnorm_1(decoder_input, lambda decoder_input: self.masked_multihead_attention(decoder_input, decoder_input, decoder_input, decoder_mask))\n",
        "    # Masked Multihead Attention output from skip connection and Cross Multihead Attention block\n",
        "    decoder_input = self.addnorm_2(decoder_input, lambda decoder_input: self.cross_multihead_attention(decoder_input, encoder_output, encoder_output, encoder_mask))\n",
        "    # Cross Multihead Attention output from skip connection and Feed Forward block\n",
        "    decoder_input = self.addnorm_3(decoder_input, self.feed_forward)\n",
        "    return decoder_input"
      ],
      "metadata": {
        "id": "5Y0T2hWWFS6z"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple decoder blocks and layer normalization\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, decoderblocklist: nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.decoderblocklist = decoderblocklist\n",
        "    self.layer_norm = LayerNorm()\n",
        "\n",
        "  def forward(self, decoder_input, encoder_output, encoder_mask, decoder_mask):\n",
        "    # Loop input through all decoder blocks\n",
        "    for decoderblock in self.decoderblocklist:\n",
        "      decoder_input = decoderblock(decoder_input, encoder_output, encoder_mask, decoder_mask)\n",
        "    # Normalize the final decoder block output\n",
        "    decoder_output = self.layer_norm(decoder_input)\n",
        "    return decoder_output"
      ],
      "metadata": {
        "id": "y2Zcl1xiHzWw"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear layer and softmax activation\n",
        "class ProjectionLayer(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "    super().__init__()\n",
        "    self.projection_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, decoder_output):\n",
        "    # output: batch_size x seq_len x vocab_size\n",
        "    output = self.projection_layer(decoder_output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "a7KyqoUlIrYJ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "v5rlc7mPKKVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full transformer model; encodes embeddings, decodes outputs, and projects predictions\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder: Encoder, decoder: Decoder, source_embed: EmbeddingLayer, target_embed: EmbeddingLayer, source_pos: PositionalEncoding, target_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "    super().__init__()\n",
        "    # Encode\n",
        "    self.source_embed = source_embed\n",
        "    self.source_pos = source_pos\n",
        "    self.encoder = encoder\n",
        "\n",
        "    # Decode\n",
        "    self.target_embed = target_embed\n",
        "    self.target_pos = target_pos\n",
        "    self.decoder = decoder\n",
        "\n",
        "    # Maps decoder output to vocabulary\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "  def encode(self, encoder_input, encoder_mask):\n",
        "    encoder_input = self.source_embed(encoder_input)\n",
        "    encoder_input = self.source_pos(encoder_input)\n",
        "    encoder_output = self.encoder(encoder_input, encoder_mask)\n",
        "    return encoder_output\n",
        "\n",
        "  def decode(self, encoder_output, encoder_mask, decoder_input, decoder_mask):\n",
        "    decoder_input = self.target_embed(decoder_input)\n",
        "    decoder_input = self.target_pos(decoder_input)\n",
        "    decoder_output = self.decoder(decoder_input, encoder_output, encoder_mask, decoder_mask)\n",
        "    return decoder_output\n",
        "\n",
        "  def project(self, decoder_output):\n",
        "    return self.projection_layer(decoder_output)"
      ],
      "metadata": {
        "id": "U-aAhfIsKLRI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(source_vocab_size, target_vocab_size, source_seq_len, target_seq_len, d_model=512, num_blocks=6, num_heads=8, dropout_rate=0.1, d_ff=2048):\n",
        "  # Embedding layers\n",
        "  source_embed = EmbeddingLayer(d_model, source_vocab_size)\n",
        "  target_embed = EmbeddingLayer(d_model, target_vocab_size)\n",
        "\n",
        "  # Positional encoding layers\n",
        "  source_pos = PositionalEncoding(d_model, source_seq_len, dropout_rate)\n",
        "  target_pos = PositionalEncoding(d_model, target_seq_len, dropout_rate)\n",
        "\n",
        "  # Encoder block list\n",
        "  encoderblocklist = []\n",
        "  for _ in range(num_blocks):\n",
        "    multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
        "    feed_forward = FeedForward(d_model, d_ff, dropout_rate)\n",
        "    encoder_block = EncoderBlock(multihead_attention, feed_forward, dropout_rate)\n",
        "    encoderblocklist.append(encoder_block)\n",
        "  # Encoder\n",
        "  encoder = Encoder(nn.ModuleList(encoderblocklist))\n",
        "\n",
        "  # Decoder block list\n",
        "  decoderblocklist = []\n",
        "  for _ in range(num_blocks):\n",
        "    masked_multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
        "    cross_multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
        "    feed_forward = FeedForward(d_model, d_ff, dropout_rate)\n",
        "    decoder_block = DecoderBlock(masked_multihead_attention, cross_multihead_attention, feed_forward, dropout_rate)\n",
        "    decoderblocklist.append(decoder_block)\n",
        "  # Decoder\n",
        "  decoder = Decoder(nn.ModuleList(decoderblocklist))\n",
        "\n",
        "  # Projection layer\n",
        "  projection_layer = ProjectionLayer(d_model, target_vocab_size)\n",
        "\n",
        "  # Transformer\n",
        "  model = Transformer(encoder, decoder, source_embed, target_embed, source_pos, target_pos, projection_layer)\n",
        "\n",
        "  # Init model params\n",
        "  for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "ZWi_S5RuLwEY"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "model = build_model(tokenizer_en.get_vocab_size(), tokenizer_my.get_vocab_size(), max_seq_len, max_seq_len, d_model=512).to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWUzQoyZNeUE",
        "outputId": "3fc3ed4f-90e3-479a-d0ff-c68f75589de8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (source_embed): EmbeddingLayer(\n",
            "    (embedding): Embedding(2028, 512)\n",
            "  )\n",
            "  (source_pos): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): Encoder(\n",
            "    (encoderblocklist): ModuleList(\n",
            "      (0-5): 6 x EncoderBlock(\n",
            "        (multihead_attention): MultiHeadAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (W_q): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (W_k): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (W_v): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (W_o): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): FeedForward(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (layer_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (layer_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        )\n",
            "        (addnorm_1): AddAndNorm(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (layer_norm): LayerNorm()\n",
            "        )\n",
            "        (addnorm_2): AddAndNorm(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (layer_norm): LayerNorm()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm()\n",
            "  )\n",
            "  (target_embed): EmbeddingLayer(\n",
            "    (embedding): Embedding(2363, 512)\n",
            "  )\n",
            "  (target_pos): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (decoderblocklist): ModuleList(\n",
            "      (0-5): 6 x DecoderBlock(\n",
            "        (masked_multihead_attention): MultiHeadAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (W_q): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (W_k): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (W_v): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (W_o): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (cross_multihead_attention): MultiHeadAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (W_q): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (W_k): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (W_v): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (W_o): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): FeedForward(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (layer_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (layer_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        )\n",
            "        (addnorm_1): AddAndNorm(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (layer_norm): LayerNorm()\n",
            "        )\n",
            "        (addnorm_2): AddAndNorm(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (layer_norm): LayerNorm()\n",
            "        )\n",
            "        (addnorm_3): AddAndNorm(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (layer_norm): LayerNorm()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm()\n",
            "  )\n",
            "  (projection_layer): ProjectionLayer(\n",
            "    (projection_layer): Linear(in_features=512, out_features=2363, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "DQW533JSN9Vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_validation(model, validation_ds, tokenizer_en, tokenizer_my, max_seq_len, device, print_msg, global_step):\n",
        "  # Change model to only evaluate\n",
        "  model.eval()\n",
        "  count = 0\n",
        "\n",
        "  # Don\"t calculate gradients during evaluation\n",
        "  with torch.no_grad():\n",
        "    for batch in validation_ds:\n",
        "      count += 1\n",
        "\n",
        "      # Get input and mask\n",
        "      encoder_input = batch[\"encoder_input\"].to(device)\n",
        "      encoder_mask = batch[\"encoder_mask\"].to(device)\n",
        "\n",
        "      # Begin and end of sentence tokens\n",
        "      cls_id = tokenizer_my.token_to_id(\"[CLS]\")\n",
        "      sep_id = tokenizer_my.token_to_id(\"[SEP]\")\n",
        "\n",
        "      # Calculate output of the encoder from the val sequence\n",
        "      encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "\n",
        "      # Decoder input first token is the beginning of sentence token\n",
        "      decoder_input = torch.empty(1, 1).fill_(cls_id).type_as(encoder_input).to(device)\n",
        "\n",
        "      # Iteratively add tokens\n",
        "      while True:\n",
        "        # Decoder input is the max length\n",
        "        if decoder_input.size(1) == max_seq_len:\n",
        "          break\n",
        "\n",
        "        # Recreate causal mask for token prediction with a new decoder input\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n",
        "\n",
        "        # Get probabilities for the next token\n",
        "        out = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "        prob = model.project(out[:, -1])\n",
        "\n",
        "        # Greedily get the next token with the highest probability\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "\n",
        "        # Add predicted token to the decoder input\n",
        "        decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(encoder_input).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "        # Next token is the end of sentence token\n",
        "        if next_word == sep_id:\n",
        "          break\n",
        "\n",
        "      model_out = decoder_input.squeeze(0)\n",
        "\n",
        "      # Get source text, target text, and predicted text\n",
        "      source_text = batch[\"source_text\"][0]\n",
        "      target_text = batch[\"target_text\"][0]\n",
        "      model_out_text = tokenizer_my.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "      print_msg(\"-\" * 55)\n",
        "      print_msg(f\"Source Text: {source_text}\")\n",
        "      print_msg(f\"Target Text: {target_text}\")\n",
        "      print_msg(f\"Predicted by MalayGPT: {model_out_text}\")\n",
        "\n",
        "      if count == 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "kBzBcKuON-VV"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "BmfPR3f3VyZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(preload_epoch=None):\n",
        "  EPOCHS = 100\n",
        "  initial_epoch = 0\n",
        "  global_step = 0\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, eps=1e-9)\n",
        "\n",
        "  # Start at preloaded epoch, weights, and optimizer\n",
        "  if preload_epoch is not None:\n",
        "    # Load model\n",
        "    model_filename = f\"./malaygpt/model_{preload_epoch}.pth\"\n",
        "    state = torch.load(model_filename)\n",
        "    model.load_state_dict(state[\"model_state_dict\"])\n",
        "    # Get initial epoch\n",
        "    initial_epoch = state[\"epoch\"] + 1\n",
        "    # Get initial optimizer\n",
        "    optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
        "    global_step = state[\"global_step\"]\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_en.token_to_id(\"[PAD]\"), label_smoothing=0.1).to(device)\n",
        "\n",
        "  for epoch in range(initial_epoch, EPOCHS):\n",
        "    # Change model to train\n",
        "    model.train()\n",
        "    # Load dataset batches\n",
        "    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "    for batch in batch_iterator:\n",
        "      # batch_size x seq_len\n",
        "      encoder_input = batch[\"encoder_input\"].to(device)\n",
        "      # batch_size x seq_len\n",
        "      decoder_input = batch[\"decoder_input\"].to(device)\n",
        "      # batch_size x 1 x 1 x seq_len\n",
        "      encoder_mask = batch[\"encoder_mask\"].to(device)\n",
        "      # batch_size x 1 x seq_len x seq_len\n",
        "      decoder_mask = batch[\"decoder_mask\"].to(device)\n",
        "      # batch_size x seq_len\n",
        "      target_label = batch[\"target_label\"].to(device)\n",
        "\n",
        "      # batch_size x seq_len x d_model\n",
        "      encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "      decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "      # batch_size x seq_len x vocab_size\n",
        "      projection_output = model.project(decoder_output)\n",
        "\n",
        "      # Calculate loss of the batch\n",
        "      loss = loss_fn(projection_output.view(-1, tokenizer_my.get_vocab_size()), target_label.view(-1))\n",
        "      batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "      global_step += 1\n",
        "\n",
        "    # Run validation after every epoch\n",
        "    run_validation(model, val_dataloader, tokenizer_en, tokenizer_my, max_seq_len, device, lambda msg: batch_iterator.write(msg), global_step)\n",
        "\n",
        "    model_filename = f\"./malaygpt/model_{epoch}.pt\"\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"global_step\": global_step\n",
        "    }, model_filename)"
      ],
      "metadata": {
        "id": "7wVXGox5Nrdf"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(preload_epoch=None)"
      ],
      "metadata": {
        "id": "Z0W0tmreV2kt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}